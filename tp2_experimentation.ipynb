{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c855039e",
   "metadata": {},
   "source": [
    "# TP 2 - Entrenamiento y evaluación de modelos\n",
    "## Formula 1 World Championship (1950 - 2023) modificado\n",
    "El dataset que usamos para entrenar y evaluar los distintos modelos es el generado en la etapa del análisis exploratorio de datos con las distintas conclusiones que pudimos sacar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a1028",
   "metadata": {},
   "source": [
    "## Métrica de performance elegida\n",
    "Para evaluar los distintos modelos con los que vamos a experimentar, decidimos centrarnos en la métrica de __Precision__.\n",
    "Esto se debe a que para el caso de uso del modelo es más importante asegurarnos de que la mayoría de los casos que etiquetamos como podio, en realidad lo sean, evitando los falsos positivos, es decir, predecir que un corredor acabará en podio pero en realidad eso no sucede.\n",
    "La otra opción era trabajar con __recall__, pero determinamos que __precision__ es útil cuando el costo de tener falsos positivos es alto y recall es útil cuando el costo de tener falsos negativos es alto.\n",
    "Por otro lado, descartamos completamente __accuracy__, ya que nuestro conjunto de datos está claramente desbalanceado hacia el lado de que el corredor no acabará en el podio. Por esto, se podría dar el caso de que el modelo prediga siempre que un corredor no acabará en el podio y tendrá una ‘puntería’ de aproximadamente 85%, como se pudo observar en la etapa de análisis exploratorio de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f73d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las dependencias que vamos a utilizar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import sklearn_pandas\n",
    "from matplotlib import gridspec\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import learning_curve\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca847242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitamos el límite de filas que se muestran en los dataframes\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b681d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importamos el dataset modificado en el EDA.\n",
    "full = pd.read_csv('./f1_race_podiums.csv')\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cce193",
   "metadata": {},
   "source": [
    "### Tratamiento de valores nulos.\n",
    "Realizamos el tratamiento de valores nulos como fue determinado en el análisis exploratorio de datos. De esta forma, tendremos los datos preparados para el entrenamiento.\n",
    "\n",
    "__Time:__ a los valores nulos de esta feature le asignamos el valor que más se repite en el set de datos.\n",
    "\n",
    "__ds_position:__ en este caso, tenemos valores nulos cuando es la primer temporada del corredor, por lo tanto no tenemos el dato de cómo finalizó la temporada anterior. Le asignamos un valor aleatorio de los disponibles en el dataset.\n",
    "\n",
    "__cs_position:__ aplicamos la misma lógica que en el anterior.\n",
    "\n",
    "__ds_points y ds_wins:__ los inicializamos en 0 ya que sería la primer carrera de la temporada.\n",
    "\n",
    "__cs_points y cs_wins:__ aplicamos la misma lógica que en el anterior.\n",
    "\n",
    "__q1_ms, q2_ms y q3_ms:__ obtenemos el mayor tiempo de vuelta registrado en la primer etapa de clasificación de una carrera, al cual le sumaremos un segundo y se lo asignaremos a los valores nulos de esa carrera para estas tres features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da104c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajamos los valores nulos detectados en el EDA\n",
    "# Null times\n",
    "moda = str(full['time'].mode()[0]) # Moda: 12:00:00\n",
    "full['time'].fillna(moda, inplace=True)\n",
    "\n",
    "# Null ds_positions\n",
    "filtrado = full[(~full.ds_position.isna())]\n",
    "# Le asignamos una posicion aleatoria en los casos que sea la primer temporada del corredor.\n",
    "full['ds_position'] = np.where(full.ds_position.isna(), filtrado.sample(1)['ds_position'], full['ds_position'])\n",
    "\n",
    "#Null cs_positions\n",
    "# Le asignamos una posicion aleatoria en los casos que sea la primer temporada del constructor.\n",
    "full['cs_position'] = np.where(full.cs_position.isna(), filtrado.sample(1)['cs_position'], full['cs_position'])\n",
    "\n",
    "# Null cs/ds_points and cs/ds_wins\n",
    "# Los nulos que quedan en ds/cs points y wins, ya sean porque es la primer carrera de la temporada o la primer\n",
    "# carrera del piloto/escuderia, se inicia en 0\n",
    "full['cs_points'].fillna(0, inplace=True)\n",
    "full['cs_wins'].fillna(0, inplace=True)\n",
    "full['ds_points'].fillna(0, inplace=True)\n",
    "full['ds_wins'].fillna(0, inplace=True)\n",
    "\n",
    "# Q1, Q2 and Q3 null lap times\n",
    "# Nos guardamos el mayor tiempo registrado en cada Q1 para luego asignarlo a los valores nulos de Q1, Q2 y Q3 de esa carrera\n",
    "# sumandole 1 segundo para diferenciarlo del máximo tiempo obtenido.\n",
    "max_q1_times = full.groupby('date')['q1_ms'].max()\n",
    "full['q1_ms'] = full['q1_ms'].fillna(full['date'].map(max_q1_times)+1000)\n",
    "full['q2_ms'] = full['q2_ms'].fillna(full['date'].map(max_q1_times)+1000)\n",
    "full['q3_ms'] = full['q3_ms'].fillna(full['date'].map(max_q1_times)+1000)\n",
    "\n",
    "# Detectamos que hay un caso particular de una carrera donde no se tienen los datos de la clasificación\n",
    "# A estos casos (24), decidimos eliminarlos\n",
    "full = full.drop(full[(full.q1_ms.isna()) & (full.q2_ms.isna()) & (full.q3_ms.isna())].index)\n",
    "\n",
    "full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin feature engineering, hacemos la división del dataset para luego evaluar.\n",
    "# 60% train, 20% test, 20% validation\n",
    "train_no_fe, not_train_no_fe = train_test_split(full, train_size=0.6, random_state=42)\n",
    "validation_no_fe, test_no_fe = train_test_split(not_train_no_fe, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d7ba6",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "__podiums_amnt:__ creación de feature que determina la cantidad de podios del corredor a partir de carreras anteriores.\n",
    "\n",
    "__experience:__ creación de feature que clasifica la experiencia del corredor en posibles valores de ‘low’, ‘intermediate’ y ‘high’ a partir de la cantidad de carreras hasta la fecha. Una vez clasificado, al ser una variable categórica aplicamos la técnica ‘one-hot encoder’.\n",
    "\n",
    "__day, month, hour y minute:__ features cradas a partir de variables existentes 'time' y 'date'. De la primera realizamos la extracción de la hora y el minuto y en el caso de la fecha extraemos el día y el mes. Esto lo realizamos para poder escalar las variables ya que eran del tipo datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4de436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "# podiums_amount\n",
    "# ordenamos los valores del dataset por driverId y fecha. Acumulamos la cantidad de podios hasta el momento de la carrera.\n",
    "full = full.sort_values(['driverId', 'date'])\n",
    "full['acum_podiums'] = full.groupby('driverId')['is_podium'].cumsum()\n",
    "full['podiums_amnt'] = full['acum_podiums'] - full['is_podium']\n",
    "full = full.drop('acum_podiums', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ed097",
   "metadata": {},
   "outputs": [],
   "source": [
    "full[(full.driverId == 1) & (full.year == 2022)] # Consultamos los valores y verificamos si la cantidad es correcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience\n",
    "# usamos una columna auxiliar para ir acumulando la cantidad de carreras\n",
    "full['aux'] = 1\n",
    "\n",
    "# Obtenemos la moda de la cantidad de carreras por temporada y usamos este dato para determinar la experiencia según\n",
    "# X cantidad de temporadas en la F1\n",
    "races = pd.read_csv('./csvs/races.csv')\n",
    "races.groupby('year')['round'].max().mode()\n",
    "\n",
    "# Acumulamos las carreras y le asignamos la experiencia según cantidad aproximada de temporadas en F1\n",
    "full['acum_races'] = full.groupby('driverId')['aux'].cumsum()\n",
    "full['experience'] = np.where((full['acum_races'] - 1) <= 16, 'low', np.where((full['acum_races'] - 1) <= 64, 'intermediate', 'high'))\n",
    "\n",
    "# Si el corredor tiene hasta 16 carreras, consideramos que tiene baja experiencia.\n",
    "# Si el corredor tiene entre 16 y 64 carreras, consideramos que tiene experiencia intermedia.\n",
    "# Si el corredor tiene más de 64 carreras, consideramos que tiene alta experiencia.\n",
    "\n",
    "# Verificamos el dato obtenido con un corredor en particular\n",
    "full[(full.driverId == 1)]\n",
    "\n",
    "# Quitamos columnas auxiliares\n",
    "full = full.drop(['aux', 'acum_races'], axis=1)\n",
    "\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a98676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering de date y time.\n",
    "# Separamos estas features en más features para luego poder escalarlas\n",
    "full['date'] = pd.to_datetime(full['date'])\n",
    "full['day'] = full['date'].dt.day\n",
    "full['month'] = full['date'].dt.month\n",
    "full['time'] = pd.to_datetime(full['time'], format= '%H:%M:%S')\n",
    "full['hour'] = full['time'].dt.hour\n",
    "full['minute'] = full['time'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2076dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42078e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya tenemos todos los datos trabajados y no hay ningún valor nulo. Pasamos al entrenamiento de los modelos.\n",
    "# Quitamos el driverId del dataset. También los campos date y time que ya fueron explotados en más campos.\n",
    "full.drop('driverId', axis=1, inplace=True)\n",
    "full.drop('time', axis=1, inplace=True)\n",
    "full.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35d8ac",
   "metadata": {},
   "source": [
    "## Separación del dataset en train, test y validation\n",
    "Aplicamos la técnica de Hold Out para dividir el dataset. Utilizamos un 60% para train, y lo restante lo dividimos equitativamente entre test y validation.\n",
    "Hacer esto nos sirve para aplicar las métricas a estos subconjuntos de datos y poder verificar si el modelo está generalizando bien o está incurriendo en overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8303748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% test, 20% validation\n",
    "train, not_train = train_test_split(full, train_size=0.6, random_state=42)\n",
    "validation, test = train_test_split(not_train, train_size=0.5, random_state=42)\n",
    "\n",
    "full.shape, train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ed867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18937a66",
   "metadata": {},
   "source": [
    "### Mapper y transformer\n",
    "Utilizamos el DataFrameMapper para facilitar la tarea de comunicar Pandas con Sklearn. Para cada feature del dataset, indicamos los transformer a aplicar. Como fue comentado en el análisis exploratorio de datos, usaremos Standar Scaler para variables numéricas y One Hot Encoder para las variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236827f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el mapper y transformer a aplicar en cada columna\n",
    "mapper = DataFrameMapper([\n",
    "    (['grid'], [StandardScaler()]),\n",
    "    (['q_position'], [StandardScaler()]),\n",
    "    (['year'], [StandardScaler()]),\n",
    "    (['ds_points'], [StandardScaler()]),\n",
    "    (['ds_position'], [StandardScaler()]),\n",
    "    (['ds_wins'], [StandardScaler()]),\n",
    "    (['cs_points'], [StandardScaler()]),\n",
    "    (['cs_position'], [StandardScaler()]),\n",
    "    (['cs_wins'], [StandardScaler()]),\n",
    "    (['q1_ms'], [StandardScaler()]),\n",
    "    (['q2_ms'], [StandardScaler()]),\n",
    "    (['q3_ms'], [StandardScaler()]),\n",
    "    (['podiums_amnt'], [StandardScaler()]),\n",
    "    (['day'], [StandardScaler()]),\n",
    "    (['month'], [StandardScaler()]),\n",
    "    (['hour'], [StandardScaler()]),\n",
    "    (['minute'], [StandardScaler()]),\n",
    "    (['circuitRef'], [OneHotEncoder()]),\n",
    "    (['experience'], [OneHotEncoder()])\n",
    "])\n",
    "\n",
    "# Trained with train\n",
    "mapper.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac018207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos las features finales luego de aplicar StandarScaler y OneHotEncoder\n",
    "mapper.transformed_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper para algoritmos que utilizan árboles\n",
    "dtree_mapper = DataFrameMapper([\n",
    "    (['grid'], None),\n",
    "    (['q_position'], None),\n",
    "    (['year'], None),\n",
    "    (['ds_points'], None),\n",
    "    (['ds_position'], None),\n",
    "    (['ds_wins'], None),\n",
    "    (['cs_points'], None),\n",
    "    (['cs_position'], None),\n",
    "    (['cs_wins'], None),\n",
    "    (['q1_ms'], None),\n",
    "    (['q2_ms'], None),\n",
    "    (['q3_ms'], None),\n",
    "    (['podiums_amnt'], None),\n",
    "    (['day'], None),\n",
    "    (['month'], None),\n",
    "    (['hour'], None),\n",
    "    (['minute'], None),\n",
    "    (['circuitRef'], [OneHotEncoder()]),\n",
    "    (['experience'], [OneHotEncoder()])\n",
    "])\n",
    "dtree_mapper.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper para los modelos sin FE\n",
    "mapper_no_fe = DataFrameMapper([\n",
    "    (['grid'], [StandardScaler()]),\n",
    "    (['q_position'], [StandardScaler()]),\n",
    "    (['year'], [StandardScaler()]),\n",
    "    (['ds_points'], [StandardScaler()]),\n",
    "    (['ds_position'], [StandardScaler()]),\n",
    "    (['ds_wins'], [StandardScaler()]),\n",
    "    (['cs_points'], [StandardScaler()]),\n",
    "    (['cs_position'], [StandardScaler()]),\n",
    "    (['cs_wins'], [StandardScaler()]),\n",
    "    (['q1_ms'], [StandardScaler()]),\n",
    "    (['q2_ms'], [StandardScaler()]),\n",
    "    (['q3_ms'], [StandardScaler()]),\n",
    "    (['circuitRef'], [OneHotEncoder()]),\n",
    "])\n",
    "\n",
    "# Trained with train\n",
    "mapper_no_fe.fit(train_no_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_mapper_no_fe = DataFrameMapper([\n",
    "    (['grid'], None),\n",
    "    (['q_position'], None),\n",
    "    (['year'], None),\n",
    "    (['ds_points'], None),\n",
    "    (['ds_position'], None),\n",
    "    (['ds_wins'], None),\n",
    "    (['cs_points'], None),\n",
    "    (['cs_position'], None),\n",
    "    (['cs_wins'], None),\n",
    "    (['q1_ms'], None),\n",
    "    (['q2_ms'], None),\n",
    "    (['q3_ms'], None),\n",
    "    (['circuitRef'], [OneHotEncoder()]),\n",
    "])\n",
    "dtree_mapper_no_fe.fit(train_no_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para evaluar distintas métricas de los modelos entrenados.\n",
    "# Gracias fisa.\n",
    "def evaluate_model(model, set_names=('train', 'validation'), title='', show_cm=False):\n",
    "    if title:\n",
    "        display(title)\n",
    "        \n",
    "    final_metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': [],\n",
    "        'AUC_ROC':[],\n",
    "    }\n",
    "        \n",
    "    for i, set_name in enumerate(set_names):\n",
    "        assert set_name in ['train', 'validation', 'test']\n",
    "        set_data = globals()[set_name]  # <- hack feo...\n",
    "\n",
    "        y = set_data.is_podium\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Accuracy'].append(metrics.accuracy_score(y, y_pred))\n",
    "        final_metrics['Precision'].append(metrics.precision_score(y, y_pred))\n",
    "        final_metrics['Recall'].append(metrics.recall_score(y, y_pred))\n",
    "        final_metrics['F1'].append(metrics.f1_score(y, y_pred))\n",
    "        final_metrics['AUC_ROC'].append(metrics.roc_auc_score(y, y_pred))\n",
    "        \n",
    "        if show_cm:\n",
    "            cm = metrics.confusion_matrix(y, y_pred)\n",
    "            cm_plot = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                                     display_labels=['Not podium', 'Podium'])\n",
    "            cm_plot.plot(cmap=\"Blues\")\n",
    "        \n",
    "    display(pd.DataFrame(final_metrics, index=set_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56005f",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "Realizamos una exploración de hiper-parámetros para cada uno de los modelos que seleccionamos para entrenar.\n",
    "Esta exploración, fue realizada a través de la técnica de búsqueda en grilla, donde le indicamos los parámetros a modificar y los posibles valores que pueden tomar.\n",
    "Para elegir los parámetros a tunear y los posibles valores que pueden tomar, nos basamos en la documentación de cada uno de estos modelos de sklearn.\n",
    "Hacer búsqueda de hiper-parámetros es buena práctica para reducir el overfitting en los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715b0e4",
   "metadata": {},
   "source": [
    "### Grid Search - Logistic Regression\n",
    "__penalty:__ tipo de regularización, la regularización L1 puede ayudar a reducir la complejidad del modelo. L2 ayuda a evitar el sobreajuste y mejorar el rendimiento del modelo.\n",
    "\n",
    "__C:__ valor de regularización, valores pequeños pueden ayudar a evitar el sobreajuste, mientras que valores grandes pueden ayudar a mejorar el ajuste del modelo pero aumenta la chance de sobreajuste.\n",
    "\n",
    "__Solver:__ es el método de solución, el algoritmo que utiliza para el problema de optimización.\n",
    "\n",
    "__max_iter:__ cantidad de veces que el modelo se entrena en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cddf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "\"\"\"\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_parameters = {\n",
    "    'penalty':['l1', 'l2'],\n",
    "    'C':[0.1,1,10],\n",
    "    'solver':['liblinear', 'saga', 'lbfgs'],\n",
    "    'max_iter':[3000, 3500]\n",
    "}\n",
    "\n",
    "lr_clf = GridSearchCV(lr_model, lr_parameters, refit=True, verbose=1)\n",
    "\n",
    "lr_gs_pipeline = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', lr_clf),\n",
    "])\n",
    "\n",
    "lr_gs_pipeline.fit(train, train.is_podium)\n",
    "lr_clf.best_score_, lr_clf.best_params_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_params = {'C': 1, 'max_iter': 3500, 'penalty': 'l1', 'solver': 'saga'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b3ef4",
   "metadata": {},
   "source": [
    "### Grid Search - K Nearest Neighbors\n",
    "__n_neighbors:__ número de vecinos a evaluar.\n",
    "\n",
    "__weights:__ especifica cómo se ponderará la contribución de los vecinos cercanos. En uniform todos los vecinos contribuyen por igual, mientras que en distance los vecinos más cercano tienen más influencia.\n",
    "\n",
    "__leaf_size:__ asignamos distintos valores para ver cuál de estos aumenta la precisión.\n",
    "\n",
    "__p:__ para utilizar la distancia Manhattan o distancia euclideana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\"\"\"\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[3,5,10,15,20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'leaf_size':[5,10,20,30,40],\n",
    "    'p':[1,2],\n",
    "}\n",
    "\n",
    "knn_clf = GridSearchCV(knn_model, knn_parameters, refit=True, verbose=1)\n",
    "knn_gs_pipeline = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', knn_clf),\n",
    "])\n",
    "\n",
    "knn_gs_pipeline.fit(train, train.is_podium)\n",
    "knn_clf.best_score_, knn_clf.best_params_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf66c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best_params = {'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'uniform'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be288c0",
   "metadata": {},
   "source": [
    "### Grid Search - Decision Tree\n",
    "__Criterion:__ especifica la función para medir la calidad de una división.\n",
    "\n",
    "__max_depth:__ profundidad máxima del árbol.\n",
    "\n",
    "__min_samples_split:__ especifica el número mínimo de muestras requeridas para dividir un nodo interno.\n",
    "\n",
    "__min_samples_leaf:__ especifica el número mínimo de muestras requeridas para formar una hoja.\n",
    "\n",
    "__max_features:__ especifica el número máximo de features que se deben considerar al buscar la mejor división en cada nodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dtree_model = DecisionTreeClassifier(random_state=42)\n",
    "dtree_parameters = {\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dtree_clf = GridSearchCV(dtree_model, dtree_parameters, refit=True, verbose=1)\n",
    "dtree_gs_pipeline = Pipeline([\n",
    "    ('mapper', dtree_mapper),\n",
    "    ('classifier', dtree_clf),\n",
    "])\n",
    "\n",
    "dtree_gs_pipeline.fit(train, train.is_podium)\n",
    "dtree_clf.best_score_, dtree_clf.best_params_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38006c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_best_params = {'criterion': 'gini',\n",
    "                  'max_depth': 3,\n",
    "                  'max_features': None,\n",
    "                  'min_samples_leaf': 1,\n",
    "                  'min_samples_split': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d804f",
   "metadata": {},
   "source": [
    "### Grid Search - Random Forest\n",
    "Realizamos la búsqueda de los mismos parámetros que en árbol de decisión, agregando:\n",
    "__n_estimators:__ número de árboles.\n",
    "\n",
    "__bootstrap:__ si se debe o no realizar muestras con reemplazos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\"\"\"\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_parameters = {\n",
    "    'n_estimators':[100,150],\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'bootstrap':[True],\n",
    "}\n",
    "\n",
    "rf_clf = GridSearchCV(rf_model, rf_parameters, refit=True, verbose=1)\n",
    "rf_gs_pipeline = Pipeline([\n",
    "    ('mapper', dtree_mapper),\n",
    "    ('classifier', rf_clf),\n",
    "])\n",
    "\n",
    "rf_gs_pipeline.fit(train, train.is_podium)\n",
    "rf_clf.best_score_, rf_clf.best_params_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {'bootstrap': True,\n",
    "              'criterion': 'entropy',\n",
    "              'max_depth': 7,\n",
    "              'max_features': None,\n",
    "              'min_samples_leaf': 1,\n",
    "              'min_samples_split': 2,\n",
    "              'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5b3f0",
   "metadata": {},
   "source": [
    "### Grid Search - Gradient Boosting\n",
    "__n_estimators:__ número de árboles que se usarán en el ensamblado.\n",
    "\n",
    "__learning_rate:__ una tasa de aprendizaje pequeña supone una mejora más lenta pero adaptándose mejor a los datos.\n",
    "\n",
    "__max_depth:__ profundidad máxima de cada árbol en el ensamblado\n",
    "\n",
    "__loss:__ especifica la función de pérdida que se utilizara durante el proceso de entrenamiento.\n",
    "\n",
    "__max_features:__ número máximo de características que se utilizaran para dividir un nodo interno.\n",
    "\n",
    "__subsample:__ fracción de muestras que se utilizarán para entrenar cada árbol en el ensamblado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\"\"\"\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_parameters = {\n",
    "    'n_estimators':[50,100,120],\n",
    "    'learning_rate':[0.1,0.3],\n",
    "    'max_depth': [3, 5],\n",
    "    'loss':['log_loss', 'exponential'],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'subsample':[1.0,0.8],\n",
    "}\n",
    "\n",
    "gb_clf = GridSearchCV(gb_model, gb_parameters, refit=True, verbose=1)\n",
    "gb_gs_pipeline = Pipeline([\n",
    "    ('mapper', dtree_mapper),\n",
    "    ('classifier', gb_clf),\n",
    "])\n",
    "gb_gs_pipeline.fit(train, train.is_podium)\n",
    "gb_clf.best_score_, gb_clf.best_params_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best_params = {'learning_rate': 0.1,\n",
    "                  'loss': 'log_loss',\n",
    "                  'max_depth': 3,\n",
    "                  'max_features': None,\n",
    "                  'n_estimators': 50,\n",
    "                  'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd28a3",
   "metadata": {},
   "source": [
    "### Entrenamiento con Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ce293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a utilizar con mejores parámetros\n",
    "\n",
    "# Logistic Regression\n",
    "lr_best_params_model = LogisticRegression(**lr_best_params, random_state=42)\n",
    "lr_best_params_pipeline = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', lr_best_params_model),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "lr_best_params_pipeline.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"LR Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los tiempos y el algoritmo para luego analizarlo\n",
    "training_times = [['LR', stop-start]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4a632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn_best_params_model = KNeighborsClassifier(**knn_best_params)\n",
    "knn_best_params_pipeline = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('classifier', knn_best_params_model),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "knn_best_params_pipeline.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"KNN Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['KNN', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "dtree_best_params_model = DecisionTreeClassifier(**dtree_best_params, random_state=42)\n",
    "dtree_best_params_pipeline = Pipeline([\n",
    "    ('mapper', dtree_mapper),\n",
    "    ('classifier', dtree_best_params_model),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "dtree_best_params_pipeline.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Decision Tree Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ee7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['DTree', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_best_params_model = RandomForestClassifier(**rf_best_params, random_state=42)\n",
    "rf_best_params_pipeline = Pipeline([\n",
    "    ('mapper', dtree_mapper),\n",
    "    ('classifier', rf_best_params_model),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "rf_best_params_pipeline.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Random Forest Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['RF', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f758bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_best_params_model = GradientBoostingClassifier(**gb_best_params, random_state=42)\n",
    "gb_best_params_pipeline = Pipeline([\n",
    "    ('mapper', dtree_mapper),\n",
    "    ('classifier', gb_best_params_model),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "gb_best_params_pipeline.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Gradient Boosting Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['GB', stop-start])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63a4c7",
   "metadata": {},
   "source": [
    "### Evaluación de modelos con Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0be4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos los modelos entrenados\n",
    "evaluate_model(lr_best_params_pipeline, title='Logistic Regression', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b18068",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(knn_best_params_pipeline, title='KNN', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dtree_best_params_pipeline, title='Decision Tree', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ce79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_best_params_pipeline, title='Random Forest', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7c5c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(gb_best_params_pipeline, title='Gradient Boosting', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb842f",
   "metadata": {},
   "source": [
    "### Entrenamiento de modelos con FE y PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e74227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a utilizar con mejores parámetros con PCA\n",
    "# Logistic Regression w/ PCA\n",
    "lr_best_params_model_pca = LogisticRegression(**lr_best_params, random_state=42)\n",
    "lr_best_params_pipeline_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', lr_best_params_model_pca),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "lr_best_params_pipeline_pca.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"LR Training time w/ PCA: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36279948",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['LR PCA', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN w/ PCA\n",
    "knn_best_params_model_pca = KNeighborsClassifier(**knn_best_params)\n",
    "knn_best_params_pipeline_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', knn_best_params_model_pca),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "knn_best_params_pipeline_pca.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"KNN Training time w/ PCA: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17039ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['KNN PCA', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree w/PCA\n",
    "dtree_best_params_model_pca = DecisionTreeClassifier(**dtree_best_params, random_state=42)\n",
    "dtree_best_params_pipeline_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', dtree_best_params_model_pca),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "dtree_best_params_pipeline_pca.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Decision Tree Training time w/PCA: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f323cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['DTree PCA', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8568aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest w/PCA\n",
    "rf_best_params_model_pca = RandomForestClassifier(**rf_best_params, random_state=42)\n",
    "rf_best_params_pipeline_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', rf_best_params_model_pca),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "rf_best_params_pipeline_pca.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Random Forest Training time w/PCA: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['RF PCA', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7621021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting w/PCA\n",
    "gb_best_params_model_pca = GradientBoostingClassifier(**gb_best_params, random_state=42)\n",
    "gb_best_params_pipeline_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', gb_best_params_model_pca),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "gb_best_params_pipeline_pca.fit(train, train.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Gradient Boosting Training time w/PCA: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['GB PCA', stop-start])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581cde24",
   "metadata": {},
   "source": [
    "### Evaluación de modelos con FE y PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos los modelos entrenados con PCA\n",
    "evaluate_model(lr_best_params_pipeline_pca, title='Logistic Regression w/ PCA', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(knn_best_params_pipeline_pca, title='KNN w/ PCA', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dtree_best_params_pipeline, title='Decision Tree w/PCA', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca450fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_best_params_pipeline_pca, title='Random Forest', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e60ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(gb_best_params_pipeline_pca, title='Gradient Boosting w/PCA', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75980df0",
   "metadata": {},
   "source": [
    "### Entrenamiento de modelos sin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a utilizar con mejores parámetros\n",
    "\n",
    "# Logistic Regression\n",
    "lr_best_params_model_no_fe = LogisticRegression(**lr_best_params, random_state=42)\n",
    "lr_best_params_pipeline_no_fe = Pipeline([\n",
    "    ('mapper', mapper_no_fe),\n",
    "    ('classifier', lr_best_params_model_no_fe),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "lr_best_params_pipeline_no_fe.fit(train_no_fe, train_no_fe.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"LR w/o FE Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los tiempos y el algoritmo para luego analizarlo\n",
    "training_times.append(['LR w/o FE', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637176ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn_best_params_model_no_fe = KNeighborsClassifier(**knn_best_params)\n",
    "knn_best_params_pipeline_no_fe = Pipeline([\n",
    "    ('mapper', mapper_no_fe),\n",
    "    ('classifier', knn_best_params_model_no_fe),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "knn_best_params_pipeline_no_fe.fit(train_no_fe, train_no_fe.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"KNN w/o FE Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['KNN w/o FE', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "dtree_best_params_model_no_fe = DecisionTreeClassifier(**dtree_best_params, random_state=42)\n",
    "dtree_best_params_pipeline_no_fe = Pipeline([\n",
    "    ('mapper', dtree_mapper_no_fe),\n",
    "    ('classifier', dtree_best_params_model_no_fe),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "dtree_best_params_pipeline_no_fe.fit(train_no_fe, train_no_fe.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Decision Tree w/o FE Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c226f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['DTree w/o FE', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_best_params_model_no_fe = RandomForestClassifier(**rf_best_params, random_state=42)\n",
    "rf_best_params_pipeline_no_fe = Pipeline([\n",
    "    ('mapper', dtree_mapper_no_fe),\n",
    "    ('classifier', rf_best_params_model_no_fe),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "rf_best_params_pipeline_no_fe.fit(train_no_fe, train_no_fe.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Random Forest w/o FE Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd573bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['RF w/o FE', stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_best_params_model_no_fe = GradientBoostingClassifier(**gb_best_params, random_state=42)\n",
    "gb_best_params_pipeline_no_fe = Pipeline([\n",
    "    ('mapper', dtree_mapper_no_fe),\n",
    "    ('classifier', gb_best_params_model_no_fe),\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "gb_best_params_pipeline_no_fe.fit(train_no_fe, train_no_fe.is_podium)\n",
    "stop = time.time()\n",
    "print(f\"Gradient Boosting w/o FE Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ec180",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times.append(['GB w/o FE', stop-start])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdf2c",
   "metadata": {},
   "source": [
    "### Evaluación de modelos sin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos los modelos entrenados\n",
    "evaluate_model(lr_best_params_pipeline_no_fe, title='Logistic Regression w/o FE', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(knn_best_params_pipeline_no_fe, title='KNN w/o FE', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748db6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dtree_best_params_pipeline_no_fe, title='Decision Tree w/o FE', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b294c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_best_params_pipeline_no_fe, title='Random Forest w/o FE', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e88cd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(gb_best_params_pipeline_no_fe, title='Gradient Boosting w/o FE', set_names=['train', 'test', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación tiempos de entrenamiento\n",
    "df_training_times = pd.DataFrame(training_times, columns=['Model', 'Seconds'])\n",
    "df_training_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db050c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_training_times.sort_values(by=['Seconds']), x='Seconds', y='Model', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo elegido --> Decision Tree with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb27337",
   "metadata": {},
   "source": [
    "## Modelo elegido: Decision Tree con Feature Engineering\n",
    "Para realizar la elección de este modelo tuvimos en cuenta los siguientes criterios:\n",
    "\n",
    "__Métrica precision:__ de los modelos que fueron entrenando, se encuentra en los que mayor valor de esta métrica obtuvieron. De todas formas, la mayoría de modelos obtiene valores similares. Por lo tanto, para 'desempatar' esta elección tuvimos en cuenta los otros criterios.\n",
    "\n",
    "__Tiempo de entrenamiento:__ es el segundo modelo que menos tiempo llevó para entrenarse, luego de KNN por una diferencia muy pequeña como se puede observar en el gráfico anterior.\n",
    "\n",
    "__Interpretabilidad:__ el modelo de árbol de decisión es de 'caja blanca', esto quiere decir que permite inferir conocimiento de qué es lo que está pasando con solo ver el gráfico del árbol generado.\n",
    "\n",
    "__Curva de aprendizaje:__ a través del gráfico de la curva de aprendizaje que se muestra a continuación, podemos ver que el modelo tiene buena capacidad de generalización ya que a medida que el set de datos de entrenamiento crece, la métrica de precisión de Train y Validation va incrementando de manera similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8eec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "estimator=dtree_best_params_pipeline,\n",
    "X=train,\n",
    "y=train.is_podium,\n",
    "train_sizes=np.linspace(0.1, 1.0, 50),\n",
    "cv=10,\n",
    "n_jobs=-1,\n",
    "scoring='precision')\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='#0000FF', label='Training')\n",
    "plt.plot(train_sizes, test_mean, color='#FF0000', label='Validation')\n",
    "\n",
    "plt.title('Learning Curve - Decision Tree')\n",
    "plt.xlabel('Train size')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a25c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = export_graphviz(\n",
    "    dtree_best_params_model, \n",
    "    out_file=None, \n",
    "    feature_names=mapper.transformed_names_,  \n",
    "    class_names=['Not podium', 'Podium'],  \n",
    "    filled=True, \n",
    "    rounded=True,  \n",
    "    special_characters=True,\n",
    ")\n",
    "graph = graphviz.Source(graph_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0ac53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_feat_importances = pd.DataFrame({\n",
    "    'feature':mapper.transformed_names_,\n",
    "    'importance':dtree_best_params_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=True)\n",
    "\n",
    "fig = px.bar(df_feat_importances[(df_feat_importances.importance != 0)], x='importance', y='feature', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7585d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "for i in range(50):\n",
    "    test_precision, _ = train_test_split(not_train, train_size=0.5)\n",
    "    \n",
    "    y = test_precision.is_podium\n",
    "    y_pred = dtree_best_params_pipeline.predict(test_precision)\n",
    "    \n",
    "    resultados.append(metrics.precision_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acum = 0\n",
    "for value in resultados:\n",
    "    acum += value\n",
    "    \n",
    "acum/len(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e5025",
   "metadata": {},
   "source": [
    "## Métrica a presentar al cliente\n",
    "La métrica a presentar al cliente, sería la obtenida a partir de varias predicciones realizadas con el modelo en datos que no fueron utilizados para el entrenamiento, obtener el promedio de la métrica de precision para que sea más representativa y no quedarnos con la mejor obtenida, ya que estaríamos 'engañando' al cliente ya que no sería la más común.\n",
    "Con el código anterior, sacamos la conclusión de que el modelo tiene una precisión de, aproximadamente, 66%.\n",
    "Esto quiere decir que, de las veces que el modelo predice que un corredor acabará en el podio, aproximadamente el 66% de las veces será de esa manera.\n",
    "\n",
    "Aparte de la métrica, al elegir Decision Tree como modelo, se le puede mostrar el gráfico del árbol al cliente para que vea las variables que tienen mayor peso a la hora de determinar si un piloto acabará en podio o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc102112",
   "metadata": {},
   "source": [
    "##  Diagramas de dispersión donde se visualicen los aciertos y errores del mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de dispersión para ver true positives/negatives y false positives/negatives  \n",
    "scatter_y_pred = dtree_best_params_pipeline.predict(validation)\n",
    "scatter_y_pred\n",
    "\n",
    "scatter_1 = validation[['grid', 'ds_position', 'is_podium']]\n",
    "scatter_1['prediction'] = scatter_y_pred\n",
    "\n",
    "scatter_1['result'] = np.where((scatter_1.is_podium == 1) & (scatter_1.prediction == 1), 'true_positive', \n",
    "                              np.where((scatter_1.is_podium == 0) & (scatter_1.prediction == 0), 'true_negative',\n",
    "                                np.where((scatter_1.is_podium == 0) & (scatter_1.prediction == 1), 'false_positive', 'false_negative')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562eb8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.scatter(scatter_1, x='grid', y='ds_position', color='result',template='plotly_white',\n",
    "          color_discrete_map={'true_positive': 'green', 'true_negative': 'yellow', 'false_positive': 'red', 'false_negative':'blue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85cc82",
   "metadata": {},
   "source": [
    "### Conclusiones del gráfico de dispersión\n",
    "Pudimos observar que el modelo elegido (Decision Tree) clasificará siempre como podio los casos en que la posición de salida (grid) esté entre los primeros 4 puestos, cualquier caso fuera de estos los clasifica como que no hará podio.\n",
    "Está claro que la feature grid es la que mayor peso tiene.\n",
    "Habría que buscar la forma de mejorar el modelo para darle más importancia a las otras features y no basarse sólamente en la posición de salida, ya que hay muchos casos donde el corredor no sale entre estos puestos mencionados y termina haciendo podio.\n",
    "\n",
    "Nota: para observar los distintos casos, en la leyenda de los colores se puede filtrar por los que queremos ver, para que no haya superposición de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9c83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
